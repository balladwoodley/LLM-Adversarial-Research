# LLMJailbreaks
A collection of research reports and case studies on Large Language Model (LLM) alignment vulnerabilities and guardrail circumvention techniques.
